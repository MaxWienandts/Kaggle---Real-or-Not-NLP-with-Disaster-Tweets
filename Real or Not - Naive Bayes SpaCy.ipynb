{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/nlp-getting-started/overview\n",
    "\n",
    "Welcome to one of our Getting Started machine learning competitions.\n",
    "This particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don’t have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called Kaggle Notebooks.\n",
    "\n",
    "Competition Description\n",
    "Twitter has become an important communication channel in times of emergency.\n",
    "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "\n",
    "But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:\n",
    "\n",
    "\n",
    "![i](https://storage.googleapis.com/kaggle-media/competitions/tweet_screenshot.png)\n",
    "\n",
    "\n",
    "The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.\n",
    "\n",
    "In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n",
    "\n",
    "Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n",
    "\n",
    "Acknowledgments\n",
    "This dataset was created by the company figure-eight and originally shared on their ‘Data For Everyone’ website here.\n",
    "\n",
    "Tweet source: https://twitter.com/AnyOtherAnnaK/status/629195955506708480"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: https://www.kaggle.com/c/nlp-getting-started/overview/evaluation\n",
    "\n",
    "Submissions are evaluated using F1 between the predicted and expected answers.\n",
    "\n",
    "F1 is calculated as follows:\n",
    "F1 = (2 ∗ precision ∗ recall) / (precision + recall)\n",
    "where:\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "and:\n",
    "\n",
    "True Positive [TP] = your prediction is 1, and the ground truth is also 1 - you predicted a positive and that's true!\n",
    "\n",
    "False Positive [FP] = your prediction is 1, and the ground truth is 0 - you predicted a positive, and that's false.\n",
    "\n",
    "False Negative [FN] = your prediction is 0, and the ground truth is 1 - you predicted a negative, and that's false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install spacy\n",
    "!python -m spacy download en\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  \n",
       "0  1       \n",
       "1  1       \n",
       "2  1       \n",
       "3  1       \n",
       "4  1       "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  0   NaN     NaN       \n",
       "1  2   NaN     NaN       \n",
       "2  3   NaN     NaN       \n",
       "3  9   NaN     NaN       \n",
       "4  11  NaN     NaN       \n",
       "\n",
       "                                                                                               text  \n",
       "0  Just happened a terrible car crash                                                                \n",
       "1  Heard about #earthquake is different cities, stay safe everyone.                                  \n",
       "2  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all  \n",
       "3  Apocalypse lighting. #Spokane #wildfires                                                          \n",
       "4  Typhoon Soudelor kills 28 in China and Taiwan                                                     "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout = pd.read_csv(\"test.csv\")\n",
    "holdout.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      target\n",
       "count  7613.000000   7613.00000\n",
       "mean   5441.934848   0.42966   \n",
       "std    3137.116090   0.49506   \n",
       "min    1.000000      0.00000   \n",
       "25%    2734.000000   0.00000   \n",
       "50%    5408.000000   0.00000   \n",
       "75%    8146.000000   1.00000   \n",
       "max    10873.000000  1.00000   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0   \n",
      "keyword     61  \n",
      "location    2533\n",
      "text        0   \n",
      "target      0   \n",
      "dtype: int64\n",
      "\n",
      "id          0.000000\n",
      "keyword     0.008013\n",
      "location    0.332720\n",
      "text        0.000000\n",
      "target      0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Verify missing\n",
    "print(train.isnull().sum())\n",
    "print()\n",
    "print(train.isnull().sum() / len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN            61\n",
       "fatalities     45\n",
       "deluge         42\n",
       "armageddon     42\n",
       "body%20bags    41\n",
       "damage         41\n",
       "harm           41\n",
       "sinking        41\n",
       "twister        40\n",
       "collided       40\n",
       "Name: keyword, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['keyword'].value_counts(dropna = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                2533\n",
       "USA                104 \n",
       "New York           71  \n",
       "United States      50  \n",
       "London             45  \n",
       "Canada             29  \n",
       "Nigeria            28  \n",
       "UK                 27  \n",
       "Los Angeles, CA    26  \n",
       "India              24  \n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['location'].value_counts(dropna = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'location' is not a godd variable.\n",
    "1/3 is null. Moreover there ar a lot of, '?', '??', '???' ...\n",
    "\n",
    "If we decide to use this variable, it will be necessary to handle abreviations first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are wrong labels in the train dataset.\n",
    "We will not do anything about it. All dataset have some errors and we hope to do a consistent model that will not be significantly affected by those errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>328</td>\n",
       "      <td>annihilated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ready to get annihilated for the BUCS game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>443</td>\n",
       "      <td>apocalypse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Short Reading\\n\\nApocalypse 21:1023 \\n\\nIn the spirit the angel took me to the top of an enormous high mountain and... http://t.co/v8AfTD9zeZ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>513</td>\n",
       "      <td>army</td>\n",
       "      <td>Studio</td>\n",
       "      <td>But if you build an army of 100 dogs and their leader is a lion all dogs will fight like a lion.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>2619</td>\n",
       "      <td>crashed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My iPod crashed..... \\n#WeLoveYouLouis \\n#MTVHottest One Direction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>3640</td>\n",
       "      <td>desolation</td>\n",
       "      <td>Quilmes , Arg</td>\n",
       "      <td>This desperation dislocation\\nSeparation condemnation\\nRevelation in temptation\\nIsolation desolation\\nLet it go and so to find away</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>3900</td>\n",
       "      <td>devastated</td>\n",
       "      <td>PG Chillin!</td>\n",
       "      <td>Man Currensy really be talkin that talk... I'd be more devastated if he had a ghostwriter than anybody else....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>4342</td>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Going to a fest? Bring swimming goggles for the dust storm in the circle pit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>5781</td>\n",
       "      <td>forest%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Campsite recommendations \\nToilets /shower \\nPub \\nFires \\nNo kids \\nPizza shop \\nForest \\nPretty stream \\nNo midges\\nNo snakes\\nThanks ??</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>6552</td>\n",
       "      <td>injury</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>My prediction for the Vikings game this Sunday....dont expect a whole lot. Infact I think Zimmer goal is....injury free 1st game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>6554</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dante Exum's knee injury could stem Jazz's hoped-for surge back to ... http://t.co/8PIFutrB5U</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4622</th>\n",
       "      <td>6570</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Sport_EN Just being linked to Arsenal causes injury.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4713</th>\n",
       "      <td>6701</td>\n",
       "      <td>lava</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>Imagine a room with walls that are lava lamps.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>6702</td>\n",
       "      <td>lava</td>\n",
       "      <td>probably watching survivor</td>\n",
       "      <td>The sunset looked like an erupting volcano .... My initial thought was the Pixar short Lava http://t.co/g4sChqFEsT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732</th>\n",
       "      <td>6729</td>\n",
       "      <td>lava</td>\n",
       "      <td>Clayton, NC</td>\n",
       "      <td>Check out my Lava lamp dude ???? http://t.co/To9ViqooFv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4820</th>\n",
       "      <td>6861</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>i'm a Citizen of the World</td>\n",
       "      <td>If abortion is murder then blowjobs are cannibalism and masturbation is mass genocide.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>7226</td>\n",
       "      <td>natural%20disaster</td>\n",
       "      <td>on to the next adventure</td>\n",
       "      <td>Of course the one day I have to dress professionally aka unsensibly for class is the day I have try and outrun a natural disaster!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             keyword                    location  \\\n",
       "229   328   annihilated         NaN                          \n",
       "301   443   apocalypse          NaN                          \n",
       "356   513   army                Studio                       \n",
       "1822  2619  crashed             NaN                          \n",
       "2536  3640  desolation          Quilmes , Arg                \n",
       "2715  3900  devastated          PG Chillin!                  \n",
       "3024  4342  dust%20storm        chicago                      \n",
       "4068  5781  forest%20fires      NaN                          \n",
       "4609  6552  injury              Saint Paul                   \n",
       "4611  6554  injury              NaN                          \n",
       "4622  6570  injury              NaN                          \n",
       "4713  6701  lava                Nashville, TN                \n",
       "4714  6702  lava                probably watching survivor   \n",
       "4732  6729  lava                Clayton, NC                  \n",
       "4820  6861  mass%20murder       i'm a Citizen of the World   \n",
       "5068  7226  natural%20disaster  on to the next adventure     \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "229   Ready to get annihilated for the BUCS game                                                                                                      \n",
       "301   Short Reading\\n\\nApocalypse 21:1023 \\n\\nIn the spirit the angel took me to the top of an enormous high mountain and... http://t.co/v8AfTD9zeZ   \n",
       "356   But if you build an army of 100 dogs and their leader is a lion all dogs will fight like a lion.                                                \n",
       "1822  My iPod crashed..... \\n#WeLoveYouLouis \\n#MTVHottest One Direction                                                                              \n",
       "2536  This desperation dislocation\\nSeparation condemnation\\nRevelation in temptation\\nIsolation desolation\\nLet it go and so to find away            \n",
       "2715  Man Currensy really be talkin that talk... I'd be more devastated if he had a ghostwriter than anybody else....                                 \n",
       "3024  Going to a fest? Bring swimming goggles for the dust storm in the circle pit                                                                    \n",
       "4068  Campsite recommendations \\nToilets /shower \\nPub \\nFires \\nNo kids \\nPizza shop \\nForest \\nPretty stream \\nNo midges\\nNo snakes\\nThanks ??      \n",
       "4609  My prediction for the Vikings game this Sunday....dont expect a whole lot. Infact I think Zimmer goal is....injury free 1st game                \n",
       "4611  Dante Exum's knee injury could stem Jazz's hoped-for surge back to ... http://t.co/8PIFutrB5U                                                   \n",
       "4622  @Sport_EN Just being linked to Arsenal causes injury.                                                                                           \n",
       "4713  Imagine a room with walls that are lava lamps.                                                                                                  \n",
       "4714  The sunset looked like an erupting volcano .... My initial thought was the Pixar short Lava http://t.co/g4sChqFEsT                              \n",
       "4732  Check out my Lava lamp dude ???? http://t.co/To9ViqooFv                                                                                         \n",
       "4820  If abortion is murder then blowjobs are cannibalism and masturbation is mass genocide.                                                          \n",
       "5068  Of course the one day I have to dress professionally aka unsensibly for class is the day I have try and outrun a natural disaster!              \n",
       "\n",
       "      target  \n",
       "229   1       \n",
       "301   1       \n",
       "356   1       \n",
       "1822  1       \n",
       "2536  1       \n",
       "2715  1       \n",
       "3024  1       \n",
       "4068  1       \n",
       "4609  1       \n",
       "4611  1       \n",
       "4622  1       \n",
       "4713  1       \n",
       "4714  1       \n",
       "4732  1       \n",
       "4820  1       \n",
       "5068  1       "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exemples of wrong labels\n",
    "wrong_labels = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "train[train['id'].isin(wrong_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>898</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>907</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>916</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       keyword location                        text  target\n",
       "624  898  bioterrorism  NaN      To fight bioterrorism sir.  0     \n",
       "630  907  bioterrorism  NaN      To fight bioterrorism sir.  1     \n",
       "634  916  bioterrorism  NaN      To fight bioterrorism sir.  0     "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify duplicates\n",
    "duplicated_text_boolean = train.duplicated(['text'], keep = False) \n",
    "duplicated_text = train[['id', 'text', 'target']][duplicated_text_boolean]\n",
    "\n",
    "dups_labels = [898, 907, 916]\n",
    "train[train['id'].isin(dups_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data base has not only duplicates but alse duplicated text with differents target value.\n",
    "Maybe the difference in the target value is because a difference in the location. However, if this is the case we would need more data to fix the problem.\n",
    "First we will remove all duplicates with different target value.\n",
    "After we will remove the duplicated text, staying with only the first observations that appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove different target values.\n",
    "duplicated_text.sort_values(['text', 'target'], inplace = True)\n",
    "duplicated_text_diff_taget_id = []\n",
    "\n",
    "#Save one id of text with different target.\n",
    "for i in range(1, len(duplicated_text)):\n",
    "    if (duplicated_text['text'].iloc[i] == duplicated_text['text'].iloc[i - 1]) and (duplicated_text['target'].iloc[i] != duplicated_text['target'].iloc[i - 1]):\n",
    "        duplicated_text_diff_taget_id.append(duplicated_text['id'].iloc[i])\n",
    "\n",
    "duplicated_text_diff_taget = duplicated_text[duplicated_text['id'].isin(duplicated_text_diff_taget_id)]\n",
    "duplicated_text_diff_taget_id = []\n",
    "#Save all ids of text with different target.\n",
    "for i in range(len(duplicated_text)):\n",
    "    for j in range(len(duplicated_text_diff_taget)):\n",
    "        if duplicated_text['text'].iloc[i] == duplicated_text_diff_taget['text'].iloc[j]:\n",
    "            duplicated_text_diff_taget_id.append(duplicated_text['id'].iloc[i])\n",
    "\n",
    "train = train[~train['id'].isin(duplicated_text_diff_taget_id)]\n",
    "\n",
    "\n",
    "#Keep just the first duplicated text\n",
    "train.drop_duplicates(['text'], inplace = True, keep = 'first') \n",
    "train.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the text\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing URLs. Most of the URL does not have any meaning. At least, only by their names it is impossible to know its significance.\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    #We removed @ and # from the list below because @ may imply a location or person and words with # may have different meaning. \n",
    "    punctuations = '!?+&*[]-%.,:\\/();$=><|{}^û_1234567890' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, '')\n",
    " \n",
    "    #Remove stop words and tokenization\n",
    "    text_hashtag = re.sub(r'#(\\w+)',r'HashTagHashTag\\1',text)  #Keep # words\n",
    "    filtered_text=[]\n",
    "    \n",
    "    for word in nlp(text_hashtag):\n",
    "        if (word.is_stop == False) and (word.text not in ('s', 'w', '#')) and (word.text.isspace() == False): #SpaCy has a bug with blank spaces.\n",
    "            word = word.lemma_      #Lemmatization\n",
    "            word = re.sub(r'HashTagHashTag','#',word) \n",
    "            filtered_text.append(word)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>[deeds, reason, #earthquake, allah, forgive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>[residents, asked, shelter, place, notified, officers, evacuation, shelter, place, orders, expected]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>[people, receive, #wildfires, evacuation, orders, california]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, ruby, #alaska, smoke, #wildfires, pours, school]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id keyword location  \\\n",
       "0  0      1   NaN     NaN       \n",
       "1  1      4   NaN     NaN       \n",
       "2  2      5   NaN     NaN       \n",
       "3  3      6   NaN     NaN       \n",
       "4  4      7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target  \\\n",
       "0  1        \n",
       "1  1        \n",
       "2  1        \n",
       "3  1        \n",
       "4  1        \n",
       "\n",
       "                                                                                        tokenized_clean  \n",
       "0  [deeds, reason, #earthquake, allah, forgive]                                                          \n",
       "1  [forest, fire, near, la, ronge, sask, canada]                                                         \n",
       "2  [residents, asked, shelter, place, notified, officers, evacuation, shelter, place, orders, expected]  \n",
       "3  [people, receive, #wildfires, evacuation, orders, california]                                         \n",
       "4  [got, sent, photo, ruby, #alaska, smoke, #wildfires, pours, school]                                   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"tokenized_clean\"] = train[\"text\"].apply(normalize_text)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shelter</th>\n",
       "      <th>place</th>\n",
       "      <th>evacuation</th>\n",
       "      <th>orders</th>\n",
       "      <th>#wildfires</th>\n",
       "      <th>california</th>\n",
       "      <th>fire</th>\n",
       "      <th>be</th>\n",
       "      <th>people</th>\n",
       "      <th>south</th>\n",
       "      <th>...</th>\n",
       "      <th>ï#hannaph</th>\n",
       "      <th>headquarters</th>\n",
       "      <th>smells</th>\n",
       "      <th>@livingsafely</th>\n",
       "      <th>#ar</th>\n",
       "      <th>#nc</th>\n",
       "      <th>#ok</th>\n",
       "      <th>ssw</th>\n",
       "      <th>anza</th>\n",
       "      <th>glink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   shelter  place  evacuation  orders  #wildfires  california  fire  be  \\\n",
       "0  0        0      0           0       0           0           0     0    \n",
       "1  0        0      0           0       0           0           1     0    \n",
       "2  2        2      1           1       0           0           0     0    \n",
       "3  0        0      1           1       1           1           0     0    \n",
       "4  0        0      0           0       1           0           0     0    \n",
       "\n",
       "   people  south  ...  ï#hannaph  headquarters  smells  @livingsafely  #ar  \\\n",
       "0  0       0      ...  0           0             0       0              0     \n",
       "1  0       0      ...  0           0             0       0              0     \n",
       "2  0       0      ...  0           0             0       0              0     \n",
       "3  1       0      ...  0           0             0       0              0     \n",
       "4  0       0      ...  0           0             0       0              0     \n",
       "\n",
       "   #nc  #ok  ssw  anza  glink  \n",
       "0  0    0    0    0     0      \n",
       "1  0    0    0    0     0      \n",
       "2  0    0    0    0     0      \n",
       "3  0    0    0    0     0      \n",
       "4  0    0    0    0     0      \n",
       "\n",
       "[5 rows x 6239 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count words and remove any word that shows only one time\n",
    "tokenized_text = train['tokenized_clean']\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "for tokens in tokenized_text:\n",
    "    for token in tokens:\n",
    "        if token not in single_tokens:\n",
    "            single_tokens.append(token)\n",
    "        elif token in single_tokens and token not in unique_tokens:\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "            \n",
    "counts = pd.DataFrame(0, index=np.arange(len(tokenized_text)), columns=unique_tokens)\n",
    "\n",
    "for index, e in enumerate(tokenized_text):\n",
    "    for token in e:\n",
    "        if token in unique_tokens:\n",
    "            counts.iloc[index][token] += 1\n",
    "\n",
    "counts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      2181\n",
       "3      1009\n",
       "4      603 \n",
       "5      433 \n",
       "6      260 \n",
       "7      190 \n",
       "8      164 \n",
       "9      137 \n",
       "10     106 \n",
       "11     79  \n",
       "12     89  \n",
       "13     73  \n",
       "14     57  \n",
       "15     48  \n",
       "16     58  \n",
       "17     41  \n",
       "18     30  \n",
       "19     41  \n",
       "20     28  \n",
       "21     29  \n",
       "22     29  \n",
       "23     25  \n",
       "24     21  \n",
       "25     20  \n",
       "26     24  \n",
       "27     20  \n",
       "28     23  \n",
       "29     22  \n",
       "30     27  \n",
       "31     28  \n",
       "32     22  \n",
       "33     20  \n",
       "34     21  \n",
       "35     25  \n",
       "36     19  \n",
       "37     17  \n",
       "38     17  \n",
       "39     9   \n",
       "40     12  \n",
       "41     11  \n",
       "42     8   \n",
       "43     10  \n",
       "44     16  \n",
       "45     4   \n",
       "46     8   \n",
       "47     5   \n",
       "48     4   \n",
       "49     8   \n",
       "50     2   \n",
       "51     4   \n",
       "52     7   \n",
       "53     2   \n",
       "54     1   \n",
       "55     3   \n",
       "56     5   \n",
       "57     2   \n",
       "58     2   \n",
       "59     1   \n",
       "60     5   \n",
       "61     2   \n",
       "63     2   \n",
       "64     2   \n",
       "65     1   \n",
       "66     2   \n",
       "67     2   \n",
       "68     2   \n",
       "70     3   \n",
       "71     1   \n",
       "72     3   \n",
       "73     3   \n",
       "74     1   \n",
       "75     1   \n",
       "76     1   \n",
       "80     1   \n",
       "82     1   \n",
       "83     3   \n",
       "84     2   \n",
       "85     3   \n",
       "86     1   \n",
       "87     1   \n",
       "90     1   \n",
       "92     2   \n",
       "95     2   \n",
       "96     2   \n",
       "97     2   \n",
       "99     2   \n",
       "102    1   \n",
       "103    1   \n",
       "106    1   \n",
       "109    1   \n",
       "110    1   \n",
       "111    2   \n",
       "113    1   \n",
       "115    1   \n",
       "116    1   \n",
       "118    1   \n",
       "121    1   \n",
       "123    2   \n",
       "137    1   \n",
       "139    1   \n",
       "151    1   \n",
       "156    1   \n",
       "187    1   \n",
       "225    1   \n",
       "244    1   \n",
       "298    1   \n",
       "312    1   \n",
       "340    1   \n",
       "500    1   \n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify words frequency\n",
    "word_counts = counts.sum(axis=0)\n",
    "word_counts.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "california    111\n",
       "fire          244\n",
       "be            312\n",
       "people        187\n",
       "man           109\n",
       "police        137\n",
       "like          340\n",
       "know          111\n",
       "amp           298\n",
       "time          121\n",
       "not           500\n",
       "got           123\n",
       "pm            102\n",
       "crash         116\n",
       "going         103\n",
       "emergency     151\n",
       "new           225\n",
       "day           113\n",
       "video         156\n",
       "burning       106\n",
       "news          118\n",
       "body          123\n",
       "suicide       110\n",
       "storm         115\n",
       "disaster      139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[word_counts > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore any word with a counting less than 5 to prevent overfitting. \n",
    "counts = counts.loc[:,(word_counts >= 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train.copy()\n",
    "\n",
    "train_1['index_original'] = train_1['index']\n",
    "train_1 = train_1.drop('index', axis = 1)\n",
    "\n",
    "train_1['target_label'] = train_1['target']\n",
    "train_1 = train_1.drop('target', axis = 1) #This is necessary because there is the word target in text.\n",
    "\n",
    "train_1['id_original'] = train_1['id']\n",
    "train_1 = train_1.drop('id', axis = 1)\n",
    "\n",
    "train_1['keyword_original'] = train_1['keyword']\n",
    "train_1 = train_1.drop('keyword', axis = 1)\n",
    "\n",
    "train_1['location_original'] = train_1['location']\n",
    "train_1 = train_1.drop('location', axis = 1)\n",
    "\n",
    "train_1['text_original'] = train_1['text']\n",
    "train_1 = train_1.drop('text', axis = 1)\n",
    "\n",
    "train_1 = pd.concat([train_1, counts], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4297\n",
       "1    3188\n",
       "Name: target_label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1['target_label'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5740814963259853\n",
      "0.4259185036740147\n",
      "34859\n",
      "28804\n",
      "2446\n"
     ]
    }
   ],
   "source": [
    "#First model (Naive Bayes)\n",
    "vocabulary = counts.columns\n",
    "\n",
    "p_false = train_1['target_label'].value_counts(normalize = True, dropna = False)[0]\n",
    "p_real = train_1['target_label'].value_counts(normalize = True, dropna = False)[1]\n",
    "\n",
    "# Isolating false and real desaster messages\n",
    "false_messages = train_1[train_1['target_label'] == 0]   \n",
    "real_messages = train_1[train_1['target_label'] == 1]\n",
    "\n",
    "# N_false\n",
    "n_words_false_message = false_messages['tokenized_clean'].apply(len)\n",
    "n_false = n_words_false_message.sum()\n",
    "\n",
    "# N_Ham\n",
    "n_words_real_message = real_messages['tokenized_clean'].apply(len)\n",
    "n_real = n_words_real_message.sum()\n",
    "\n",
    "# N_Vocabulary\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1\n",
    "\n",
    "print(p_false)\n",
    "print(p_real)\n",
    "print(n_false)\n",
    "print(n_real)\n",
    "print(n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate parameters\n",
    "parameters_false = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_real = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_false = false_messages[word].sum()\n",
    "    p_word_given_false = (n_word_given_false + alpha) / (n_false + alpha*n_vocabulary)\n",
    "    parameters_false[word] = p_word_given_false\n",
    "    \n",
    "    n_word_given_real = real_messages[word].sum()\n",
    "    p_word_given_real = (n_word_given_real + alpha) / (n_real + alpha*n_vocabulary)\n",
    "    parameters_real[word] = p_word_given_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "\n",
    "    p_false_given_message = p_false\n",
    "    p_real_given_message = p_real\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_false:\n",
    "            p_false_given_message *= parameters_false[word]\n",
    "            \n",
    "        if word in parameters_real:\n",
    "            p_real_given_message *= parameters_real[word]\n",
    "            \n",
    "    return p_real_given_message - p_false_given_message\n",
    "    \n",
    "train_1['dif_p_real_false'] = train_1['tokenized_clean'].apply(classify)\n",
    "train_1['target_model'] = train_1.apply(lambda row: 1 if row['dif_p_real_false'] > 0 else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8038709677419354"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluation\n",
    "f1_score(train_1['target_label'], train_1['target_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the variable \"keyword\" and model with logit and random forest.\n",
    "train_2 = train_1.copy()\n",
    "\n",
    "#Count words and remove any word that shows only one time\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "for token in train_2['keyword_original']:\n",
    "    if token not in single_tokens:\n",
    "        single_tokens.append(token)\n",
    "    elif token in single_tokens and token not in unique_tokens:\n",
    "        unique_tokens.append(token)\n",
    "\n",
    "            \n",
    "counts_keywords = pd.DataFrame(0, index=np.arange(len(tokenized_text)), columns=unique_tokens)\n",
    "\n",
    "for index, e in enumerate(tokenized_text):\n",
    "    for token in e:\n",
    "        if token in unique_tokens:\n",
    "            counts_keywords.iloc[index][token] += 1\n",
    "            \n",
    "#Ignore any keyword with a counting less than 5 to prevent overfitting. \n",
    "keyword_counts = counts_keywords.sum(axis=0)\n",
    "counts_keywords = counts_keywords.loc[:,(keyword_counts >= 5)]\n",
    "vocabulary_keywords = counts_keywords.columns\n",
    "\n",
    "counts_keywords = pd.concat([train_2[['keyword_original', 'target_label']], counts_keywords], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# Isolating false and real desaster messages\n",
    "false_keyword = counts_keywords[counts_keywords['target_label'] == 0] \n",
    "real_keyword = counts_keywords[counts_keywords['target_label'] == 1]\n",
    "\n",
    "# N_false\n",
    "n_false_keyword = len(false_keyword['keyword_original'])\n",
    "\n",
    "# N_Ham\n",
    "n_real_keyword = len(real_keyword['keyword_original'])\n",
    "\n",
    "# N_Vocabulary\n",
    "n_vocabulary_keywords = len(vocabulary_keywords)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1\n",
    "\n",
    "\n",
    "\n",
    "# Initiate parameters\n",
    "parameters_false_keyword = {unique_word:0 for unique_word in vocabulary_keywords}\n",
    "parameters_real_keyword = {unique_word:0 for unique_word in vocabulary_keywords}\n",
    "\n",
    "# Calculate parameters\n",
    "for keyword in vocabulary_keywords:\n",
    "    n_keyword_given_false = false_keyword[keyword].sum()\n",
    "    p_keyword_given_false = (n_keyword_given_false + alpha) / (n_false_keyword + alpha*n_vocabulary_keywords)\n",
    "    parameters_false_keyword[keyword] = p_keyword_given_false\n",
    "    \n",
    "    n_keyword_given_real = real_keyword[keyword].sum()\n",
    "    p_keyword_given_real = (n_keyword_given_real + alpha) / (n_real_keyword + alpha*n_vocabulary)\n",
    "    parameters_real_keyword[keyword] = p_keyword_given_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_keyword(keyword):\n",
    "\n",
    "    p_false_given_keyword = p_false\n",
    "    p_real_given_keyword = p_real\n",
    "\n",
    "    if keyword in parameters_false_keyword:\n",
    "        p_false_given_keyword *= parameters_false_keyword[keyword]\n",
    "            \n",
    "    if keyword in parameters_real_keyword:\n",
    "        p_real_given_keyword *= parameters_real_keyword[keyword]\n",
    "            \n",
    "    return p_real_given_keyword - p_false_given_keyword\n",
    "    \n",
    "train_2['dif_keyword_p_real_false'] = train_2['keyword_original'].apply(classify_keyword)\n",
    "train_2['target_model_keyword'] = train_2.apply(lambda row: 1 if row['dif_keyword_p_real_false'] > 0 else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5156985871271585"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluationclassify_keyword\n",
    "f1_score(train_2['target_label'], train_2['target_model_keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "------------------\n",
      "Best Score: 0.8037160366636067\n",
      "Best Parameters: {'class_weight': 'balanced', 'fit_intercept': True, 'solver': 'newton-cg'}\n",
      "\n",
      "RandomForestClassifier\n",
      "----------------------\n",
      "Best Score: 0.8037160366636067\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 2, 'max_features': 'log2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unfortunately, the variable keyword is not very helpfull.\n",
    "#Nonetheless, lets continue with the model.\n",
    "\n",
    "def select_model(df,features):\n",
    "    \n",
    "    all_X = df[features]\n",
    "    all_y = df[\"target_label\"]\n",
    "\n",
    "    # List of dictionaries, each containing a model name,\n",
    "    # it's estimator and a dict of hyperparameters\n",
    "    models = [\n",
    "        {\n",
    "            \"name\": \"LogisticRegression\",\n",
    "            \"estimator\": LogisticRegression(random_state = 0),\n",
    "            \"hyperparameters\":\n",
    "                {\n",
    "                    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"],\n",
    "                    \"fit_intercept\": [True, False],\n",
    "                    \"class_weight\":[\"balanced\", None]\n",
    "                }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"RandomForestClassifier\",\n",
    "            \"estimator\": RandomForestClassifier(n_estimators = 300, random_state=1),\n",
    "            \"hyperparameters\":\n",
    "                {\n",
    "                    \"criterion\": [\"entropy\", \"gini\"],\n",
    "                    \"max_depth\": [2, 3, 4, 5],\n",
    "                    \"max_features\": [\"log2\", \"sqrt\"]\n",
    "                }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for model in models:\n",
    "        print(model['name'])\n",
    "        print('-'*len(model['name']))\n",
    "\n",
    "        grid = GridSearchCV(model[\"estimator\"],\n",
    "                            param_grid=model[\"hyperparameters\"],\n",
    "                            cv=10,\n",
    "                            scoring = make_scorer(f1_score))\n",
    "        grid.fit(all_X,all_y)\n",
    "        model[\"best_params\"] = grid.best_params_\n",
    "        model[\"best_score\"] = grid.best_score_\n",
    "        model[\"best_model\"] = grid.best_estimator_\n",
    "\n",
    "        print(\"Best Score: {}\".format(model[\"best_score\"]))\n",
    "        print(\"Best Parameters: {}\\n\".format(model[\"best_params\"]))\n",
    "\n",
    "    return models\n",
    "\n",
    "columns_models = ['target_model_keyword', 'target_model']\n",
    "result = select_model(train_2, columns_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "------------------\n",
      "Best Score: 0.42310589687588074\n",
      "Best Parameters: {'class_weight': 'balanced', 'fit_intercept': False, 'solver': 'newton-cg'}\n",
      "\n",
      "RandomForestClassifier\n",
      "----------------------\n",
      "Best Score: 0.6596675475688312\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 5, 'max_features': 'log2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use continuous variables to model\n",
    "columns_models = ['dif_p_real_false', 'dif_keyword_p_real_false']\n",
    "result = select_model(train_2, columns_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seens that the best model is the first naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0  0   1     \n",
       "1  2   1     \n",
       "2  3   1     \n",
       "3  9   1     \n",
       "4  11  1     "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean and tokennize the text from the holdout\n",
    "holdout[\"tokenized_clean\"] = holdout[\"text\"].apply(normalize_text)\n",
    "\n",
    "#score the holdout\n",
    "holdout['dif_p_real_false'] = holdout['tokenized_clean'].apply(classify)\n",
    "holdout['target'] = holdout.apply(lambda row: 1 if row['dif_p_real_false'] > 0 else 0, axis = 1)\n",
    "holdout_ids = holdout[['id', 'target']]\n",
    "\n",
    "holdout_ids.to_csv(\"submission.csv\",index=False)\n",
    "holdout_ids.head()\n",
    "\n",
    "#Kaggle score: 0.77811"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
